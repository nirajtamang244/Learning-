{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Activation Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU function (Rectified Linear Unit) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU: 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ReLU Activation Function\n",
    "def relu(x):\n",
    "    return np.maximum(0,x) # checks if x is greater than 0, if yes prints x and if no prints 0.\n",
    "\n",
    "# Example input\n",
    "x = np.array([-2, 0, 2, 4])\n",
    "print(\"ReLU:\", relu(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function \n",
    "\n",
    "* Used for probability based output \n",
    "* used in spam detection, image classification\n",
    "* always between 0 and 1\n",
    "\n",
    "\n",
    "* not used for very large or very small data as the gradient becomes very small and hence slow learning \n",
    "* not used in deep learning \n",
    "* vaninshing gradient problem because fo very small/large data, each layer will have smaller gradient due to sigmoid and first layer stops learning becaus of the gradients are almost zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid: [0.11920292 0.5        0.88079708 0.98201379]\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid Activation Function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Example input\n",
    "x = np.array([-2, 0, 2, 4])\n",
    "print(\"Sigmoid:\", sigmoid(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax activation function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax: [0.00214401 0.0158422  0.11705891 0.86495488]\n"
     ]
    }
   ],
   "source": [
    "# Softmax Activation Function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Prevents numerical overflow\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# Example input\n",
    "x = np.array([-2, 0, 2, 4])\n",
    "print(\"Softmax:\", softmax(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Without Stabilization: [0.8360188  0.11314284 0.05083836]\n",
      "Softmax With Stabilization: [0.8360188  0.11314284 0.05083836]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample input\n",
    "x = np.array([3.0, 1.0, 0.2])\n",
    "\n",
    "# Softmax without stabilization (could cause overflow for large x)\n",
    "softmax_unstable = np.exp(x) / np.sum(np.exp(x))\n",
    "print(\"Softmax Without Stabilization:\", softmax_unstable)\n",
    "\n",
    "# Softmax with stabilization (numerically stable)\n",
    "softmax_stable = np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))\n",
    "print(\"Softmax With Stabilization:\", softmax_stable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if the array inputs were large ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Without Stabilization: [nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/07/tbqv_jsx71l44mqfwtg5ntjc0000gn/T/ipykernel_1021/1020786406.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  softmax_unstable = np.exp(x) / np.sum(np.exp(x))\n",
      "/var/folders/07/tbqv_jsx71l44mqfwtg5ntjc0000gn/T/ipykernel_1021/1020786406.py:7: RuntimeWarning: invalid value encountered in divide\n",
      "  softmax_unstable = np.exp(x) / np.sum(np.exp(x))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample input\n",
    "x = np.array([30000, 10000, 20000])\n",
    "\n",
    "# Softmax without stabilization (could cause overflow for large x)\n",
    "softmax_unstable = np.exp(x) / np.sum(np.exp(x))\n",
    "print(\"Softmax Without Stabilization:\", softmax_unstable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax With Stabilization: [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = np.array([30000, 10000, 20000])\n",
    "# Softmax with stabilization (numerically stable)\n",
    "softmax_stable = np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))\n",
    "print(\"Softmax With Stabilization:\", softmax_stable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.     3.    -0.025 -0.012]\n"
     ]
    }
   ],
   "source": [
    "# changing to leaky ReLU Function\n",
    "\n",
    "\n",
    "def leaky_ReLU(x):\n",
    "    return np.where(x>0,x,0.01*x)\n",
    "\n",
    "x= np.array([2.0,3.0,-2.5,-1.2])\n",
    "print(leaky_ReLU(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " output for input 1 : [0.01714783 0.04661262 0.93623955]\n",
      " output for input 2 : [7.31058579e-01 8.66351835e-28 2.68941421e-01]\n",
      " output for input 3 : [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# putting different inoput in the function and asking for output\n",
    "def soft_max(x):\n",
    "    variable= np.exp(x-np.max(x))\n",
    "    return variable/np.sum(variable)\n",
    "\n",
    "input1= np.array([1,2,5])\n",
    "input2= np.array([2,-60,1])\n",
    "input3= np.array([2000,-3000,4000])\n",
    "\n",
    "\n",
    "print(f\" output for input 1 : {soft_max(input1)}\")\n",
    "print(f\" output for input 2 : {soft_max(input2)}\")\n",
    "print(f\" output for input 3 : {soft_max(input3)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
